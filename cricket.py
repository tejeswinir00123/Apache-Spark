from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder.getOrCreate()
rawDF = spark.read.json("cricket.json", multiLine = "true")
innings=rawDF.select(explode("innings").alias("innings"))
#topDF = innings.select(col("innings.team"),explode("innings.overs.deliveries").alias("overs"),col("overs.batter"),col("overs.bowler"))

#top1DF=topDF.select(explode("overs.batter").alias("batter"),explode("overs.bowler").alias("bowler"))
top1DF=innings.select(col("innings.team"),explode("innings.overs.deliveries").alias("batbowl"))
topDF=top1DF.select(col("batbowl.batter").alias("batter"),col("batbowl.bowler").alias("bowler"))
top2DF=topDF.select(col("batter"),col("bowler"))
#length=len(top2DF.select('batter').take(1)[0][0])
#top2DF.select([top2DF.batter]+[top2DF.batter[i] for i in range(length)]).unique().show()
top6DF=top2DF.withColumn("batter",arrays_zip("batter")).select("bowler",explode("batter").alias("merge")).select("bowler",col("merge.batter"))
runs1=innings.withColumn("deliveries",explode("innings.overs.deliveries")).withColumn("runs",explode("deliveries.runs")).select("runs.total")
df1=top6DF.withColumn("id",monotonically_increasing_id())
df2=runs1.withColumn("id",monotonically_increasing_id())
df3=df2.join(df1,"id","outer").drop("id")
#df4=df3.groupBy("batter","bowler").sum().alias("total")
#df4=df3.groupBy("batter","bowler").agg(sum("total").alias("total"))
df8=df3.groupBy("batter","bowler").agg(sum("total"))
#df8.show(df8.count())
df5=df8.orderBy("batter",ascending=False)
#top3DF=top2DF.drop_duplicates(['bowler'])
top3DF=df5.withColumn("result",array_distinct("bowler"))
#top4DF=top3DF.withColumn("result1",array_distinct("batter"))
#top2DF.select("batter",top2DF.element[0],top2DF.element[1]).show()
#top3DF.show()
top3DF.show(top3DF.count())
#print((top3DF.count(), len(top3DF.column)))
